{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "tp1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xb6yqaV3CeR"
      },
      "source": [
        "ENV = 'colab'\n",
        "# ENV = 'local'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdDXez_npHLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec53e42f-b846-47b0-c836-6f1eb72a704b"
      },
      "source": [
        "if 'colab' in ENV:\n",
        "  !pip install --upgrade matplotlib"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: matplotlib in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxp742TObPMp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "a7f91d16-bdd9-46f9-d0dc-7b08a6142094"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy   as np\n",
        "import pandas  as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image  as mpimg\n",
        "from   six               import StringIO  \n",
        "from   IPython.display   import Image\n",
        "import pydotplus\n",
        "\n",
        "from sklearn.experimental    import enable_iterative_imputer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, \\\n",
        "                                    RandomizedSearchCV\n",
        "from sklearn.impute          import IterativeImputer\n",
        "from sklearn.preprocessing   import LabelEncoder\n",
        "from sklearn.tree            import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.metrics         import accuracy_score, confusion_matrix, \\\n",
        "                                    plot_confusion_matrix, \\\n",
        "                                    f1_score, precision_score, \\\n",
        "                                    recall_score, make_scorer, fbeta_score\n",
        "\n",
        "from imblearn.over_sampling  import SMOTENC\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline       import Pipeline"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5aec1ac18f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfbeta_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m  \u001b[0;32mimport\u001b[0m \u001b[0mSMOTENC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m       \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \"\"\"\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'six' from 'sklearn.externals' (/usr/local/lib/python3.7/dist-packages/sklearn/externals/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3HCBgi5bPMs"
      },
      "source": [
        "# Trabajo Practico 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km6W_uSwbPMt"
      },
      "source": [
        "Configuracion común:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHEwBU1obPMt"
      },
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "tiny_size = lambda: sns.set(rc={'figure.figsize':(5,5)})\n",
        "normal_size = lambda: sns.set(rc={'figure.figsize':(10, 6)})\n",
        "big_size = lambda: sns.set(rc={'figure.figsize':(20,5)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yiLz9QqbPMt"
      },
      "source": [
        "## Pasos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEH16QlSbPMt"
      },
      "source": [
        "#### A) A partir de los datos entregados, describir los atributos realizando una breve explicación de qué representan y del tipo de variable (categórica, numérica u ordinal). En caso de que haya variables no numéricas, reportar los posibles valores que toman y cuán frecuentemente lo hacen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM6XCCA6bPMu"
      },
      "source": [
        "if 'colab' in ENV:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    dataset = pd.read_csv('./healthcare-dataset-stroke-data.csv')\n",
        "else:\n",
        "    dataset = pd.read_csv('../dataset/healthcare-dataset-stroke-data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KM1pX0rbPMu"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGajZ_7-bPMv"
      },
      "source": [
        "#### Genero\n",
        "\n",
        "Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjHYmUskbPMv"
      },
      "source": [
        "dataset.gender.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mb5h-6VbPMw"
      },
      "source": [
        "dataset['gender'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL7T3NADbPMw"
      },
      "source": [
        "#### Edad\n",
        "\n",
        "Es una variable ordinal categorica:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZw2e-PybPMw"
      },
      "source": [
        "normal_size()\n",
        "sns.histplot(data=dataset, x='age', kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPRW-CbhbPMw"
      },
      "source": [
        "dataset['age'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trbJH6cZbPMx"
      },
      "source": [
        "#### Hypertension\n",
        "\n",
        "Indica si el paciente tiene hipertension o no. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xevhn-klbPMx"
      },
      "source": [
        "dataset['hypertension'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG7s7vrfbPMx"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='hypertension')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQE-yPPCbPMx"
      },
      "source": [
        "dataset['hypertension'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCRZey0xbPMy"
      },
      "source": [
        "#### Heart Disease\n",
        "\n",
        "Explica si el individio tiene una enfermedad cardiaca o no. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4LlRaTubPMy"
      },
      "source": [
        "dataset['heart_disease'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-jw1qMebPMy"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='heart_disease')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZto63V-bPMy"
      },
      "source": [
        "dataset['heart_disease'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4-6gyAIbPMz"
      },
      "source": [
        "#### Ever Married\n",
        "\n",
        "El individuo estuvo o esta casado?. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnEvAmbSbPMz"
      },
      "source": [
        "dataset['ever_married'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUSmCybebPMz"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='ever_married')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIyH4B84bPMz"
      },
      "source": [
        "dataset['ever_married'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9ClpQl3bPM0"
      },
      "source": [
        "#### Work Type\n",
        "\n",
        "Tipo de trabajo. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nMwbdmFbPM0"
      },
      "source": [
        "dataset['work_type'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC-e96BDbPM0"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='work_type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHCL-dImbPM0"
      },
      "source": [
        "dataset['work_type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpkN1ImUbPM0"
      },
      "source": [
        "#### Residence type\n",
        "\n",
        "Tipo de zona donde reside el invididuo. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNf9XCcrbPM1"
      },
      "source": [
        "dataset['Residence_type'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zGIKlY1bPM1"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='Residence_type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCZSW-eZbPM1"
      },
      "source": [
        "dataset['Residence_type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sAksWzjbPM1"
      },
      "source": [
        "#### AVG Glucose Level\n",
        "\n",
        "Nivel de grucosa medio el individuo. Es una variable numerica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAmKEQ14bPM1"
      },
      "source": [
        "dataset['avg_glucose_level'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_-ON24IbPM2"
      },
      "source": [
        "normal_size()\n",
        "sns.histplot(data=dataset, x='avg_glucose_level', kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06PSeW0QbPM2"
      },
      "source": [
        "#### BMI\n",
        "\n",
        "Es una variable numerica. Body Mass Index (Peso / Altura^2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2b0IdsVbPM2"
      },
      "source": [
        "normal_size()\n",
        "sns.histplot(data=dataset, x='bmi', kde= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPUbtq5NbPM2"
      },
      "source": [
        "#### Smoking Status\n",
        "\n",
        "Se refiere al nivel de fumador al que perteneces el individuo. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaLpdsJ3bPM2"
      },
      "source": [
        "dataset['smoking_status'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3BSCNyebPM3"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='smoking_status')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiZueofubPM3"
      },
      "source": [
        "dataset['smoking_status'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLShHMrhbPM3"
      },
      "source": [
        "#### Stroke\n",
        "\n",
        "Informa si el individuo sufrio un derrame cerebral o no. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btjL2tjJbPM3"
      },
      "source": [
        "dataset['stroke'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-WtYkd-bPM3"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='stroke')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGuxt8T0bPM4"
      },
      "source": [
        "dataset['stroke'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN4pxTLlbPM4"
      },
      "source": [
        "#### b) Reportar si hay valores faltantes. ¿Cuántos son y en qué atributos se encuentran? En caso de haberlos, ¿es necesario y posible asignarles un valor?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLnNl-vmbPM4"
      },
      "source": [
        "Antes de completar valores faltante vamos a separar el dataset en los conjuntos en dos conjuntos, development, validation y test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxvJSYbWbPM4"
      },
      "source": [
        "def missing_values_summary(df):\n",
        "    result = round(df.isna().sum() * 100 / len(dataset), 2)\n",
        "    result = result[result > 0]\n",
        "    result = result.apply(lambda value: f'{value}%')\n",
        "    return result \n",
        "\n",
        "def set_summary(features, target, title=None):\n",
        "    if title:\n",
        "        print(f'\\n{title}:')\n",
        "    print('- Features shape:',  features.shape)\n",
        "    print('- Target shape:',     target.shape)\n",
        "    print('- Target classes:')\n",
        "    classes = target.value_counts(normalize=True)\n",
        "    values = classes * 100\n",
        "\n",
        "    print(\"\\t- Clase {}: {:.2f} %\".format(str(classes.index[0][0]), values.values[0]))\n",
        "    print(\"\\t- Clase {}: {:.2f} %\".format(str(classes.index[1][0]), values.values[1]))\n",
        "\n",
        "    missing = missing_values_summary(features)\n",
        "\n",
        "    if missing.empty:\n",
        "        print('- Features Missing values: No missing values!')\n",
        "    else:\n",
        "        print('- Features Missing values: ')\n",
        "        print(missing)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd5s4NyvbPM5"
      },
      "source": [
        "def num_column_names(df):\n",
        "    return df.select_dtypes(include=np.number).columns\n",
        "\n",
        "def cat_column_names(df):\n",
        "    return set(df.columns) - set(num_column_names(df))\n",
        "\n",
        "def cat_column_indexes(df):\n",
        "    return [df.columns.get_loc(col_name) for col_name in cat_column_names(df)]\n",
        "\n",
        "def unique_column_values(df, column_name): \n",
        "    return df[column_name].value_counts().index.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uciwra3xbPM5"
      },
      "source": [
        "@dataclass\n",
        "class SetsGroup:\n",
        "    dev_features: pd.DataFrame\n",
        "    test_features: pd.DataFrame\n",
        "    dev_target: pd.DataFrame\n",
        "    test_target: pd.DataFrame\n",
        "\n",
        "    def summary(self):\n",
        "        set_summary(self.dev_features, self.dev_target, title='Development Set')\n",
        "        set_summary(self.test_features, self.test_target, title='Test Set')\n",
        "\n",
        "    def features(self):\n",
        "        return pd.concat([self.dev_features, self.test_features])\n",
        "    \n",
        "    def cat_feature_names(self):\n",
        "        return cat_column_names(self.dev_features)\n",
        "\n",
        "    def cat_feature_indexes(self):\n",
        "        return cat_column_indexes(self.dev_features)\n",
        "\n",
        "    def num_feature_names(self):\n",
        "        return num_column_names(self.dev_features)\n",
        "\n",
        "    def feature_unique_values(self, column_name):\n",
        "        return unique_column_values(self.features(), column_name)\n",
        "\n",
        "    def dev_set(self):      \n",
        "        return pd.concat([sets_group.dev_features,sets_group.dev_target], axis=1)\n",
        "\n",
        "class DevTestSpliter:\n",
        "    @staticmethod\n",
        "    def split(\n",
        "        dataset, \n",
        "        target_col = 'stroke', \n",
        "        test_size = 0.2,\n",
        "        random_state = 1\n",
        "    ):\n",
        "        features = dataset.loc[:, dataset.columns != target_col]\n",
        "        target   = dataset[[target_col]]\n",
        "\n",
        "        dev_features, test_features, dev_target, test_target = train_test_split(\n",
        "            features, \n",
        "            target, \n",
        "            test_size    = test_size,\n",
        "            random_state = random_state,\n",
        "            stratify     = target.values\n",
        "        )\n",
        "        return SetsGroup(dev_features, test_features, dev_target, test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asNjqOCgbPM6"
      },
      "source": [
        "sets_group = DevTestSpliter.split(dataset, test_size = 0.2)\n",
        "sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMTQvupHbPM6"
      },
      "source": [
        "La unica variable que tiene valores incompletos es BMI y es un 3.93%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF8d3Rd2bPM6"
      },
      "source": [
        "Ahora completamos valores faltantes y comparamos la distribucion de la columna BMI antes y despues de la imputacion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o10poElsbPM6"
      },
      "source": [
        "def impute_missing_values(df, excluded = ['id'], random_state=0):\n",
        "    num_features = df.select_dtypes(include=np.number)\n",
        "    num_features = num_features[set(num_features.columns) - set(excluded)]\n",
        "\n",
        "    # Algoritmo basado en el algoritmo MICE de R\n",
        "    imputer = IterativeImputer(random_state=random_state) \n",
        "    imputer.fit(num_features)\n",
        "\n",
        "    imp_num_features = imputer.transform(num_features)\n",
        "\n",
        "    result_df = pd.DataFrame(\n",
        "        data    = imp_num_features, \n",
        "        columns = num_features.columns\n",
        "    )\n",
        "    for col in cat_column_names(df):\n",
        "        result_df[col] = df[col].values\n",
        "\n",
        "    return result_df\n",
        "\n",
        "class MissingValuesImpoter:\n",
        "    def __init__(self, excluded = ['id'], random_state=0):\n",
        "        self.excluded     = excluded\n",
        "        self.random_state = random_state\n",
        "    \n",
        "    def impute(self, sets_group):\n",
        "        imp_dev_features  = impute_missing_values(\n",
        "            sets_group.dev_features,\n",
        "            random_state = self.random_state\n",
        "        )\n",
        "        imp_test_features = impute_missing_values(\n",
        "            sets_group.test_features,\n",
        "            random_state = self.random_state\n",
        "        )\n",
        "        return SetsGroup(\n",
        "            imp_dev_features, \n",
        "            imp_test_features, \n",
        "            sets_group.dev_target, \n",
        "            sets_group.test_target\n",
        "        )\n",
        "\n",
        "\n",
        "def compare_distributions(df1, df2, column):\n",
        "    sns.kdeplot(df1[column], shade=True, color=\"r\")\n",
        "    sns.kdeplot(df2[column], shade=True, color=\"b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB5usAaybPM6"
      },
      "source": [
        "Chequeamos que se hayan completado als columnas y comparamos las distribuciones para development y test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP45je9tbPM7"
      },
      "source": [
        "imputer = MissingValuesImpoter()\n",
        "imp_sets_group = imputer.impute(sets_group)\n",
        "imp_sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gSkoJi0bPM7"
      },
      "source": [
        "compare_distributions(imp_sets_group.dev_features, sets_group.dev_features, 'bmi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHGfORfHbPM7"
      },
      "source": [
        "compare_distributions(imp_sets_group.test_features, sets_group.test_features, 'bmi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT23ATFBbPM7"
      },
      "source": [
        "Ahora la columna BMI esta completa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t26WcfFDbPM7"
      },
      "source": [
        "#### c) ¿Qué variables se correlacionan más con el evento de lesión (Stroke)? Para las cuatro más correlacionadas, realizar un gráfico en el que se pueda observar la correlación entre la variable y el stroke."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uzDUs0ybPM8"
      },
      "source": [
        "def heatmap(corr, resumed=True, figsize=(10, 10), vmax=.3, square=True, annot=True, linewidths=3):\n",
        "    if resumed:\n",
        "        mask = np.zeros_like(corr)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "    else:\n",
        "        mask = None\n",
        "\n",
        "    with sns.axes_style(\"white\"):\n",
        "        f, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(corr, mask=mask, vmax=vmax, square=square, annot=annot, linewidths=linewidths)\n",
        "\n",
        "        \n",
        "tmp_dataset = dataset.drop('id', axis=1)\n",
        "heatmap(tmp_dataset.corr())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgAPhIFobPM8"
      },
      "source": [
        "Al parecer stroke tiene baja correlacion con las demas variables pero existe un nivel. El orden de mas correlacionada a menos es:\n",
        "\n",
        "* Age (0.25)\n",
        "* Hypertension, Heart Disease y AVG Glucose Level (0.13)\n",
        "* BMI (0.044)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7vvYrBybPM8"
      },
      "source": [
        "**Edad vs. dañoi cerebral**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q68-sK52bPM8"
      },
      "source": [
        "sns.pairplot(tmp_dataset[['age', 'stroke']], height=3)\n",
        "sns.pairplot(tmp_dataset[['age','stroke']], hue=\"stroke\", height=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDoiyli-bPM9"
      },
      "source": [
        "Conclusiones:\n",
        "\n",
        "* Ambas densidades estan solapadas.\n",
        "* Para age <= 23: La probabilidad de tener un daño cereblar es practicamente nula.\n",
        "* Para age > 23: La probabilidad de tener un daño cereblar es conciderable pero sigue siendo baja con relaciona a la probabilidad de no tener daño cerebral."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNMd61IBbPM9"
      },
      "source": [
        "**Hipertensión vs daño cerebral**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ2z-O7kbPM9"
      },
      "source": [
        "sns.pairplot(tmp_dataset[['hypertension', 'stroke']], height=3)\n",
        "sns.pairplot(tmp_dataset[['hypertension','stroke']], hue=\"stroke\", height=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aj-x6qIbPM9"
      },
      "source": [
        "Conclusiones:\n",
        "    \n",
        "* Ambas densidades estan solapadas.\n",
        "* -0.2 <= hypertension <= 0.2\n",
        "    * P(Daño cerebral/ Nivel de hipertension) <<<< P(Daño cerebral/ Nivel de hipertension)\n",
        "* 0.8 <= hypertension <= 1.2\n",
        "    * P(Daño cerebral/ Nivel de hipertension) <<<< P(No Daño cerebral/ Nivel de hipertension)\n",
        "* En otros valores hay una probabilidad muy baja de daño cereblar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psna_UMdbPM-"
      },
      "source": [
        "**Enfermedad del corazón vs daño cerebral**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpnjkbP5bPM-"
      },
      "source": [
        "sns.pairplot(tmp_dataset[['heart_disease', 'stroke']], height=3)\n",
        "sns.pairplot(tmp_dataset[['heart_disease','stroke']], hue=\"stroke\", height=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdIPQbGDbPM-"
      },
      "source": [
        "Conclusiones:\n",
        "    \n",
        "* Ambas densidades estan solapadas.\n",
        "* -0.2 <= heart_disease <= 0.2\n",
        "    * P(Daño cerebral/ Enfermedad del corazón) <<<< P(No Daño cerebral/ Enfermedad del corazón)\n",
        "* 0.8.5 <= hypertension <= 1.1\n",
        "    * P(Daño cerebral/ Enfermedad del corazón) <<<< P(No Daño cerebral/ Enfermedad del corazón)\n",
        "* En otros valores hay una probabilidad muy baja de daño cereblar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLZ7EpOpbPM-"
      },
      "source": [
        "**Promedio del nivel de glucosa vs daño cerebral**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKXUEbHBbPM-"
      },
      "source": [
        "sns.pairplot(tmp_dataset[['avg_glucose_level', 'stroke']], height=3)\n",
        "sns.pairplot(tmp_dataset[['avg_glucose_level','stroke']], hue=\"stroke\", height=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdqRfKBMbPM_"
      },
      "source": [
        "Conclusiones:\n",
        "    \n",
        "* Ambas densidades estan solapadas.\n",
        "* - 40 <= avg_glucose_level <= 170\n",
        "    * P(Daño cerebral/ nivel de glucosa) <<<< P(No Daño cerebral/ nivel de glucosa)\n",
        "* 0.8.5 <= hypertension <= 1.1\n",
        "    * P(Daño cerebral/ nivel de glucosa) <<<< P(No Daño cerebral/ nivel de glucosa)\n",
        "* En otros valores hay una probabilidad muy baja de daño cereblar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2kS1tqwbPM_"
      },
      "source": [
        "#### d) Se necesita saber cuáles son los indicadores que determinan más susceptibilidad a sufrir una lesión. ¿Qué atributos utilizará como variables predictoras? ¿Por qué?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qaV8HlubPM_"
      },
      "source": [
        "Completar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nHicTnFbPM_"
      },
      "source": [
        "#### e) ¿Se encuentra balanceado el conjunto de datos que utilizará para desarrollar el algoritmo diseñado para contestar el punto d)? En base a lo respondido, ¿qué métricas de performance reportaría y por qué? En caso de estar desbalanceado, ¿qué estrategia de balanceo utilizaría?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKAgUH1WbPM_"
      },
      "source": [
        "imp_sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZQpv_IKbPM_"
      },
      "source": [
        "**Claramente esta desbalanceado** dado que es normal tener menos casos de daño cerebral."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeOQVL38bPNA"
      },
      "source": [
        "class OverUnderSampler:    \n",
        "    def __init__(\n",
        "        self,\n",
        "        categorical_features, \n",
        "        random_state=None, \n",
        "        oversampling_strategy='auto', \n",
        "        undersampling_strategy='auto'\n",
        "    ):\n",
        "        oversampler = SMOTENC(\n",
        "            categorical_features = categorical_features, \n",
        "            random_state = random_state,\n",
        "            sampling_strategy = undersampling_strategy\n",
        "        )\n",
        "        undersampler = RandomUnderSampler(sampling_strategy = undersampling_strategy)\n",
        "        self.pipeline = Pipeline(steps=[('oversampler', oversampler), ('undersampler', undersampler)])\n",
        "\n",
        "    def perform(self, features, target):\n",
        "        bal_features, bal_target = self.pipeline.fit_resample(features.values, target.values)\n",
        "        \n",
        "        # Matrix to DataFrame\n",
        "        bal_features = pd.DataFrame(data = bal_features, columns = features.columns)\n",
        "        bal_target   = pd.DataFrame(data = bal_target,   columns = target.columns)\n",
        "\n",
        "        return bal_features, bal_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "oLgdstO_bPNA"
      },
      "source": [
        "class SetsGroupOverUnderSampler:\n",
        "    @staticmethod\n",
        "    def createFromHps(categorical_features, hps):\n",
        "        return SetsGroupOverUnderSampler(\n",
        "            categorical_features   = categorical_features,\n",
        "            random_state           = hps.random_state, \n",
        "            oversampling_strategy  = hps.oversampling_strategy,\n",
        "            undersampling_strategy = hps.undersampling_strategy \n",
        "        )\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        categorical_features,\n",
        "        random_state = None, \n",
        "        oversampling_strategy = 0.1,\n",
        "        undersampling_strategy = 1\n",
        "    ):\n",
        "        self.sampler = OverUnderSampler(\n",
        "            categorical_features   = categorical_features,\n",
        "            random_state           = random_state, \n",
        "            oversampling_strategy  = oversampling_strategy,\n",
        "            undersampling_strategy = undersampling_strategy\n",
        "        )\n",
        "\n",
        "    def perform(self, sets_group):\n",
        "        bal_dev_features, bal_dev_target = self.sampler.perform(\n",
        "            sets_group.dev_features, \n",
        "            sets_group.dev_target\n",
        "        )\n",
        "\n",
        "        return SetsGroup(\n",
        "            bal_dev_features, \n",
        "            sets_group.test_features, \n",
        "            bal_dev_target, \n",
        "            sets_group.test_target\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rq38pk4nY6-"
      },
      "source": [
        "sampler = SetsGroupOverUnderSampler(\n",
        "    categorical_features   = imp_sets_group.cat_feature_indexes(),\n",
        "    oversampling_strategy  = 0.1,\n",
        "    undersampling_strategy = 1\n",
        ")\n",
        "oversampled_sets_group = sampler.perform(imp_sets_group)\n",
        "oversampled_sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwA3ixDQbPNA"
      },
      "source": [
        "#### f) Suponiendo que es más importante detectar los casos en donde el evento ocurre. ¿Qué medida de performance utilizaría? Si utiliza Fβ-Score, ¿qué valor de β eligiría?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZpTMGSJbPNB"
      },
      "source": [
        "**Completar!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZJtYo5vbPNB"
      },
      "source": [
        "#### g) Implementar el algoritmo introducido en el punto d) utilizando árboles de decisión. En primer lugar, se deberá separar un 20% de los datos para usarlos como conjunto de evaluación (test set). El conjunto restante (80%) es el de desarrollo y es con el que se deberá continuar haciendo el trabajo. Realizar los siguientes puntos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xF3X_IbbPNB"
      },
      "source": [
        "class CategoricalFeaturesEncoder:\n",
        "    def perform(self, sets_group):\n",
        "        enc_dev_features  = sets_group.dev_features.copy()\n",
        "        enc_test_features = sets_group.test_features.copy()\n",
        " \n",
        "        for col_name in sets_group.cat_feature_names():            \n",
        "            encoder = LabelEncoder()\n",
        "            encoder.fit(sets_group.feature_unique_values(col_name))\n",
        "\n",
        "            enc_test_features[col_name] = encoder.transform(sets_group.test_features[col_name].values)\n",
        "            enc_dev_features[col_name]  = encoder.transform(sets_group.dev_features[col_name].values)\n",
        "\n",
        "\n",
        "        return SetsGroup(\n",
        "            enc_dev_features, \n",
        "            enc_test_features, \n",
        "            sets_group.dev_target, \n",
        "            sets_group.test_target\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTD2QZwLIR4-"
      },
      "source": [
        "encoder = CategoricalFeaturesEncoder()\n",
        "encoded_sets_group = encoder.perform(oversampled_sets_group)\n",
        "encoded_sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-PxU2A-bPNB"
      },
      "source": [
        "encoder = CategoricalFeaturesEncoder()\n",
        "encoded_sets_group = encoder.perform(encoded_sets_group)\n",
        "encoded_sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEgOG8fuIAHB"
      },
      "source": [
        "def features_importance(features, target):\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(features, target)\n",
        "    importance = model.feature_importances_\n",
        "\n",
        "    # Orden desc...\n",
        "    return np.sort(importance)[::-1]\n",
        "\n",
        "def plot_features_importance(sets_group):\n",
        "    importance = features_importance(\n",
        "        sets_group.dev_features.values, \n",
        "        sets_group.dev_target.values\n",
        "    )\n",
        "\n",
        "    column_names = sets_group.dev_features.columns\n",
        "    plt.bar([column_names[x] for x in range(len(importance))], importance)\n",
        "    plt.show()\n",
        "\n",
        "big_size()\n",
        "plot_features_importance(encoded_sets_group)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lewTlbk81kvH"
      },
      "source": [
        "Quitamos los features menos importantes para la clasificacion por stroke:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWMtPA_N2BlU"
      },
      "source": [
        "def remove_columns(df, columns): df.drop(columns, axis = 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSAWjJxnIt9P"
      },
      "source": [
        "less_importante_features = [\n",
        "    'Residence_type',\n",
        "    'smoking_status',\n",
        "    'gender',\n",
        "    'ever_married',\n",
        "    'work_type'\n",
        "]\n",
        "\n",
        "remove_columns(encoded_sets_group.dev_features, less_importante_features)\n",
        "remove_columns(encoded_sets_group.test_features, less_importante_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNprI7AmbPNC"
      },
      "source": [
        "def to_tree_img(tree):\n",
        "    dot_data = StringIO()\n",
        "    export_graphviz(\n",
        "        tree, \n",
        "        out_file            = dot_data,  \n",
        "        filled              = True,\n",
        "        rounded            = True,\n",
        "        special_characters = True\n",
        "    )\n",
        "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "    graph.write_png('tree.png')\n",
        "    return mpimg.imread('tree.png')\n",
        "\n",
        "def plot_tree(tree):\n",
        "    sns.set(rc={'figure.figsize':(15, 8)})\n",
        "    plt.title('Arbol de decisión')\n",
        "    plt.grid(False)\n",
        "    plt.imshow(to_tree_img(tree))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "def cm_plot(ax, cm, title='Matriz de confusión'):     \n",
        "    sns.heatmap(\n",
        "        cm, \n",
        "        ax=ax,\n",
        "        annot=True, \n",
        "        fmt='g', \n",
        "        cmap='Blues', \n",
        "        cbar=False\n",
        "    )\n",
        "    ax.set_ylabel('Realidad')\n",
        "    ax.set_xlabel('Predicciones')\n",
        "    ax.set_title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO1KhYKobPNC"
      },
      "source": [
        "@dataclass\n",
        "class HiperParams:\n",
        "    # Hiper parametros del modelo:\n",
        "    criterion: any = 'entropy' \n",
        "    max_depth: int = 5\n",
        "    min_samples_leaf: int = 1\n",
        "    ccp_alpha: float = 0.0\n",
        "    class_weight: any = 'balanced'\n",
        "    # Semilla usada en todos lo algoritmos.\n",
        "    random_state: int = 42 \n",
        "    # Hiper parametros para over/up sampling:\n",
        "    # Porcentaje de ejemplos duplicados en la calse minoritaria.\n",
        "    oversampling_strategy: float = 0.1\n",
        "    # Porcentaje de ejemplos removidos ne la clase mayoritaria.\n",
        "    undersampling_strategy: float = 1\n",
        "\n",
        "class ModelFactory:\n",
        "    @staticmethod\n",
        "    def create(hps):\n",
        "        tree = DecisionTreeClassifier(\n",
        "            criterion        = hps.criterion,\n",
        "            max_depth        = hps.max_depth,\n",
        "            min_samples_leaf = hps.min_samples_leaf,\n",
        "            ccp_alpha        = hps.ccp_alpha,\n",
        "            class_weight     = hps.class_weight,\n",
        "            random_state     = hps.random_state\n",
        "        )\n",
        "        return Model(tree, hps)\n",
        "\n",
        "    def create_from(tree):\n",
        "        return Model(tree, hps)\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, tree, hps):\n",
        "        self.tree = tree\n",
        "        self.hps = hps\n",
        "\n",
        "    def fit(self, sets_group):\n",
        "        return self.tree.fit(\n",
        "            sets_group.dev_features.values, \n",
        "            sets_group.dev_target.values\n",
        "        )\n",
        "\n",
        "    def predict(self, features):\n",
        "        return self.tree.predict(features.values)\n",
        "\n",
        "    def evaluate(self, sets_group):\n",
        "        train_pred  = self.predict(sets_group.dev_features)\n",
        "        test_pred   = self.predict(sets_group.test_features)\n",
        "        return ModelSummary(\n",
        "            sets_group, \n",
        "            train_pred, \n",
        "            test_pred, \n",
        "            tree = self.tree, \n",
        "            hps  = self.hps\n",
        "        )\n",
        "\n",
        "class ModelSummary:\n",
        "    def __init__(self, sets_group, train_pred, test_pred, tree, hps):\n",
        "        self.metrics = {\n",
        "            'train_accuracy':         accuracy_score(sets_group.dev_target.values, train_pred),\n",
        "            'train_precision':        precision_score(sets_group.dev_target.values, train_pred),\n",
        "            'train_recall':           recall_score( sets_group.dev_target.values, train_pred),\n",
        "            \"train_f1_score\":         f1_score(sets_group.dev_target.values, train_pred),\n",
        "            'train_confusion_matrix': confusion_matrix(sets_group.dev_target.values, train_pred),\n",
        "\n",
        "            'test_accuracy':          accuracy_score(sets_group.test_target.values, test_pred),\n",
        "            'test_precision':         precision_score(sets_group.test_target.values, test_pred),\n",
        "            'test_recall':            recall_score(sets_group.test_target.values, test_pred),\n",
        "            \"test_f1_score\":          f1_score(sets_group.test_target.values, test_pred),\n",
        "            'test_confusion_matrix':  confusion_matrix(sets_group.test_target.values, test_pred)\n",
        "        }\n",
        "        self.tree = tree\n",
        "        self.hps = hps\n",
        "\n",
        "    def plot_confusion_matrix(self):\n",
        "        sns.set(rc={'figure.figsize':(7, 4)})\n",
        "        fig = plt.figure()\n",
        "        gs = fig.add_gridspec(1, 2, hspace=0.1, wspace=0.1)\n",
        "        (ax1, ax2) = gs.subplots(sharex='col', sharey='row')\n",
        "        cm_plot(\n",
        "            ax=ax1,\n",
        "            cm=self.metrics['train_confusion_matrix'],\n",
        "            title='Entrenamiento'\n",
        "        )\n",
        "        cm_plot(\n",
        "            ax=ax2,\n",
        "            cm=self.metrics['test_confusion_matrix'],\n",
        "            title='Test'\n",
        "        )\n",
        "        fig.suptitle('Matriz de confusión')\n",
        "        plt.show()\n",
        "\n",
        "    def show_metrics(self):\n",
        "        print('\\nMetricas:')\n",
        "        for name in self.metrics.keys():\n",
        "            if \"confusion_matrix\" not in name:\n",
        "                print(f'- {name}: {self.metrics[name]*100:.2f}%')\n",
        "\n",
        "    def showHps(self):\n",
        "        if self.hps:\n",
        "            print('\\nHiper Parametros:\\n-', self.hps)\n",
        "\n",
        "    def show(self):\n",
        "        self.showHps()\n",
        "        self.show_metrics()\n",
        "        self.plot_confusion_matrix() \n",
        "        plot_tree(self.tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaUARJap1G-u"
      },
      "source": [
        "def data_transform_pipeline(data, hps):\n",
        "    # Imputamos valores faltantes (MICE like)...\n",
        "    imputer = MissingValuesImpoter(random_state = hps.random_state)\n",
        "    data = imputer.impute(data)\n",
        "    \n",
        "    # Balanceamos el dataset (over/up sampling)...\n",
        "    sampler = SetsGroupOverUnderSampler.createFromHps(\n",
        "        data.cat_feature_indexes(), \n",
        "        hps\n",
        "    )\n",
        "    data = sampler.perform(data)\n",
        "\n",
        "    # Llevamos variables categoricas a numericas...\n",
        "    endcoder = CategoricalFeaturesEncoder()\n",
        "    data = endcoder.perform(data)\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpUN03FdbPNC"
      },
      "source": [
        "hiper_params = [\n",
        "    # Usamos todos los valores por defecto...\n",
        "    HiperParams(\n",
        "        # Hiper parametros del modelo:\n",
        "        criterion        = 'entropy',\n",
        "        max_depth        = 2,\n",
        "        min_samples_leaf = 100\n",
        "    )\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjsf4R2RbPND"
      },
      "source": [
        "for hps in hiper_params:\n",
        "    data = data_transform_pipeline(sets_group, hps)\n",
        "\n",
        "    # Resumen de estrutura de los datos...\n",
        "    data.summary()\n",
        "\n",
        "    # Creamos el modelo (Arbol de decisión)...\n",
        "    model = ModelFactory.create(hps)\n",
        "\n",
        "    # Entrenamos...\n",
        "    model.fit(data)\n",
        "    summary = model.evaluate(data)\n",
        "    \n",
        "    # Mostramos el resumen de metricas...\n",
        "    summary.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykBZ1JFsbPND"
      },
      "source": [
        "#### g.1) Armar conjuntos de entrenamiento y validación con proporción 80-20 del conjunto de desarrollo de forma aleatoria. Usar 50 semillas distintas y realizar un gráfico de caja y bigotes que muestre cómo varía la métrica elegida en c) en esas 50 particiones distintas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXDSqSQ79W-o"
      },
      "source": [
        "class MetricsComparePlot:\n",
        "    def __init__(self):\n",
        "        self.accs = []\n",
        "        self.precisions = []\n",
        "        self.recalls = []\n",
        "        self.f1s = []\n",
        "\n",
        "    def add(self, y_val,y_pred_val):\n",
        "        self.accs.append(accuracy_score(y_val,y_pred_val))\n",
        "        self.precisions.append(precision_score(y_val,y_pred_val))\n",
        "        self.recalls.append(recall_score(y_val,y_pred_val))\n",
        "        self.f1s.append(f1_score(y_val,y_pred_val))\n",
        "\n",
        "    def plot(self):\n",
        "        all_metrics = self.accs + self.precisions + self.recalls + self.f1s\n",
        "        metric_labels = ['Accuracy']*len(self.accs) + ['Precision']*len(self.precisions) + ['Recall']*len(self.recalls) + ['F1 Score']*len(self.f1s)\n",
        "        sns.set_context('talk')\n",
        "        plt.figure(figsize=(15,8))\n",
        "        sns.boxplot(metric_labels, all_metrics)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3xV0j6Dpmfz"
      },
      "source": [
        "# Rearmo dataset dev (feat+tgt) y vuelvo a correr train/test \n",
        "# split, ya que el primero separó dev + val\n",
        "dev_concat = sets_group.dev_set()\n",
        "\n",
        "n_seeds   = 50\n",
        "test_size = 0.2\n",
        "plotter   = MetricsComparePlot()\n",
        "\n",
        "for seed in range(n_seeds):\n",
        "    hps = HiperParams(random_state = seed)\n",
        "\n",
        "    sets_group_dev = DevTestSpliter.split(\n",
        "        dev_concat, \n",
        "        test_size = test_size, \n",
        "        random_state = hps.random_state\n",
        "    )\n",
        "\n",
        "    encoded_sets_group_dev = data_transform_pipeline(sets_group_dev, hps)\n",
        "\n",
        "    arbol = ModelFactory.create(hps)\n",
        "    arbol = arbol.fit(encoded_sets_group_dev)\n",
        "    y_pred_val = arbol.predict(encoded_sets_group_dev.test_features)\n",
        "\n",
        "    plotter.add(encoded_sets_group_dev.test_target, y_pred_val)\n",
        "\n",
        "plotter.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d6YBNB2bPND"
      },
      "source": [
        "#### g.2) Usar validación cruzada de 50 iteraciones (50-fold cross validation). Realizar un gráfico de caja y bigotes que muestre cómo varía la métrica elegida en esas 50 particiones distintas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6sLNnKhhPDu"
      },
      "source": [
        "def search_best_model(sets_group, params_grid, n_splits, n_iter):\n",
        "    print('Random Search Hiper-params:', params_grid)\n",
        "\n",
        "    # scoring = make_scorer(f1_score)\n",
        "    scoring = make_scorer(fbeta_score, beta=0.2)\n",
        "\n",
        "    randomcv = RandomizedSearchCV(\n",
        "        estimator           = DecisionTreeClassifier(),\n",
        "        param_distributions = params_grid,\n",
        "        scoring             = scoring,\n",
        "        cv                  = StratifiedKFold(n_splits=n_splits),\n",
        "        n_iter              = n_iter,\n",
        "        return_train_score  = True\n",
        "    )\n",
        "    randomcv.fit(\n",
        "        sets_group.dev_features.values,\n",
        "        sets_group.dev_target.values\n",
        "    )\n",
        "    \n",
        "    random_cv_results = pd.DataFrame(randomcv.cv_results_)\n",
        "    print('Models Count:', random_cv_results.shape[0])\n",
        "\n",
        "    return randomcv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMvFA0ULcmv0"
      },
      "source": [
        "def plot_randomcv_metrics(randomcv_result, metrics):\n",
        "    df = pd.DataFrame(randomcv_result.cv_results_)\n",
        "    normal_size()\n",
        "    for metric in metrics:\n",
        "        plt.plot(df[metric],  linestyle='-', marker='o', label=metric)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_randomcv_score(randomcv_result):\n",
        "    plot_randomcv_metrics(randomcv_result, metrics=['mean_train_score','mean_test_score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKLoVTOQpySK"
      },
      "source": [
        "n_splits = 50\n",
        "\n",
        "randomcv = search_best_model(\n",
        "    encoded_sets_group_dev,\n",
        "    params_grid = {\n",
        "        'criterion': ['gini','entropy'],\n",
        "        'max_depth': list(range(1, 50)),\n",
        "        'min_samples_leaf': list(range(1, 50)),\n",
        "        'class_weight': ['balanced']\n",
        "    }, \n",
        "    n_splits = n_splits,\n",
        "    n_iter = 50\n",
        ")\n",
        "plot_randomcv_score(randomcv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8PFbjTjdSVI"
      },
      "source": [
        "random_cv_results = pd.DataFrame(randomcv.cv_results_)\n",
        "random_cv_results[random_cv_results.params == randomcv.best_params_]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEwsnTE_pyYT"
      },
      "source": [
        "metric_labels = []\n",
        "metric_labels = ['F1 Score']*n_splits\n",
        "\n",
        "accs_kfold = []\n",
        "for x in range(0,n_splits):\n",
        "  col = 'split' + str(x) + '_test_score'\n",
        "  accs_kfold.append(random_cv_results[col].values[0])\n",
        "\n",
        "hue = []\n",
        "hue = ['Arbol1']*n_splits\n",
        "sns.set_context('talk')\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.boxplot(metric_labels,accs_kfold,hue=hue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UlOsS_KbPND"
      },
      "source": [
        "#### h) Graficar el árbol de decisión con mejor performance encontrado en el punto g2). Analizar el árbol de decisión armado (atributos elegidos y decisiones evaluadas)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsDLc4ZnCYbj"
      },
      "source": [
        "def evaluate_model(estimator, sets_group):\n",
        "    model = ModelFactory.create_from(estimator)\n",
        "    model.fit(sets_group)\n",
        "    summary = model.evaluate(sets_group)\n",
        "    summary.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIF_za85SSdv"
      },
      "source": [
        "evaluate_model(\n",
        "    estimator  = randomcv.best_estimator_,\n",
        "    sets_group = encoded_sets_group_dev\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-uUf3qbbPNE"
      },
      "source": [
        "#### i) Usando validación cruzada de 10 iteraciones (10-fold cross validation), probar distintos valores de α del algoritmo de poda mínima de complejidad de costos (algoritmo de poda de sklearn). Hacer gráficos de la performance en validación y entrenamiento en función del α. Explicar cómo varía la profundidad de los árboles al realizar la poda con distintos valores de α."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xY8qa4hr2bl"
      },
      "source": [
        "def to_params_grid(hiper_params):\n",
        "    params_grid = {}\n",
        "    for name in hiper_params.keys():\n",
        "        params_grid[name] = [hiper_params[name]]\n",
        "    return params_grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzDhUBJQYeLc"
      },
      "source": [
        "params_grid = to_params_grid(randomcv.best_params_)\n",
        "params_grid['ccp_alpha'] = np.linspace(0, 0.3, 100)\n",
        "\n",
        "randomcv2 = search_best_model(\n",
        "    encoded_sets_group_dev,\n",
        "    params_grid = params_grid, \n",
        "    n_splits = 10,\n",
        "    n_iter = 5\n",
        ")\n",
        "\n",
        "plot_randomcv_score(randomcv2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24_SJzGlbPNE"
      },
      "source": [
        "#### j) Evaluar en el conjunto de evaluación, el árbol correspondiente al α que maximice la performance en el conjunto de validación. Comparar con el caso sin poda (α=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhhG2w6At7f-"
      },
      "source": [
        "**Modelo con poda:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk30vqcAjEvu"
      },
      "source": [
        "evaluate_model(\n",
        "    estimator  = randomcv2.best_estimator_,\n",
        "    sets_group = encoded_sets_group_dev\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgQBbax5uH-N"
      },
      "source": [
        "**Modelo sin poda**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjgB_P_zuDpg"
      },
      "source": [
        "evaluate_model(\n",
        "    estimator  = randomcv.best_estimator_,\n",
        "    sets_group = encoded_sets_group_dev\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH9O_YiYbPNE"
      },
      "source": [
        "#### k) Para el árbol sin poda, obtener la importancia de los descriptores usando la técnica de eliminación recursiva. Reentrenar el árbol usando sólo los 3 descriptores más importantes. Comparar la performance en el conjunto de prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTL_a1A9bPNE"
      },
      "source": [
        "**Completar!**"
      ]
    }
  ]
}