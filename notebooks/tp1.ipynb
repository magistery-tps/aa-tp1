{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "tp1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Xb6yqaV3CeR"
      },
      "source": [
        "ENV = 'colab'\n",
        "# ENV = 'local'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdDXez_npHLu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec53e42f-b846-47b0-c836-6f1eb72a704b"
      },
      "source": [
        "if 'colab' in ENV:\n",
        "  !pip install --upgrade matplotlib"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: matplotlib in /usr/local/lib/python3.7/dist-packages (3.4.2)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nxp742TObPMp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "a7f91d16-bdd9-46f9-d0dc-7b08a6142094"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy   as np\n",
        "import pandas  as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image  as mpimg\n",
        "from   six               import StringIO  \n",
        "from   IPython.display   import Image\n",
        "import pydotplus\n",
        "\n",
        "from sklearn.experimental    import enable_iterative_imputer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, \\\n",
        "                                    RandomizedSearchCV\n",
        "from sklearn.impute          import IterativeImputer\n",
        "from sklearn.preprocessing   import LabelEncoder\n",
        "from sklearn.tree            import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.metrics         import accuracy_score, confusion_matrix, \\\n",
        "                                    plot_confusion_matrix, \\\n",
        "                                    f1_score, precision_score, \\\n",
        "                                    recall_score, make_scorer, fbeta_score\n",
        "\n",
        "from imblearn.over_sampling  import SMOTENC\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline       import Pipeline"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5aec1ac18f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m                                     \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_scorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfbeta_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m  \u001b[0;32mimport\u001b[0m \u001b[0mSMOTENC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomUnderSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m       \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \"\"\"\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'six' from 'sklearn.externals' (/usr/local/lib/python3.7/dist-packages/sklearn/externals/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3HCBgi5bPMs"
      },
      "source": [
        "# Trabajo Practico 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km6W_uSwbPMt"
      },
      "source": [
        "Configuracion com√∫n:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHEwBU1obPMt"
      },
      "source": [
        "sns.set(style=\"darkgrid\")\n",
        "tiny_size = lambda: sns.set(rc={'figure.figsize':(5,5)})\n",
        "normal_size = lambda: sns.set(rc={'figure.figsize':(10, 6)})\n",
        "big_size = lambda: sns.set(rc={'figure.figsize':(20,5)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yiLz9QqbPMt"
      },
      "source": [
        "## Pasos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEH16QlSbPMt"
      },
      "source": [
        "#### A) A partir de los datos entregados, describir los atributos realizando una breve explicaci√≥n de qu√© representan y del tipo de variable (categ√≥rica, num√©rica u ordinal). En caso de que haya variables no num√©ricas, reportar los posibles valores que toman y cu√°n frecuentemente lo hacen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM6XCCA6bPMu"
      },
      "source": [
        "if 'colab' in ENV:\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    dataset = pd.read_csv('./healthcare-dataset-stroke-data.csv')\n",
        "else:\n",
        "    dataset = pd.read_csv('../dataset/healthcare-dataset-stroke-data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KM1pX0rbPMu"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGajZ_7-bPMv"
      },
      "source": [
        "#### Genero\n",
        "\n",
        "Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjHYmUskbPMv"
      },
      "source": [
        "dataset.gender.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mb5h-6VbPMw"
      },
      "source": [
        "dataset['gender'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL7T3NADbPMw"
      },
      "source": [
        "#### Edad\n",
        "\n",
        "Es una variable ordinal categorica:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZw2e-PybPMw"
      },
      "source": [
        "normal_size()\n",
        "sns.histplot(data=dataset, x='age', kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPRW-CbhbPMw"
      },
      "source": [
        "dataset['age'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trbJH6cZbPMx"
      },
      "source": [
        "#### Hypertension\n",
        "\n",
        "Indica si el paciente tiene hipertension o no. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xevhn-klbPMx"
      },
      "source": [
        "dataset['hypertension'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NG7s7vrfbPMx"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='hypertension')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQE-yPPCbPMx"
      },
      "source": [
        "dataset['hypertension'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCRZey0xbPMy"
      },
      "source": [
        "#### Heart Disease\n",
        "\n",
        "Explica si el individio tiene una enfermedad cardiaca o no. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4LlRaTubPMy"
      },
      "source": [
        "dataset['heart_disease'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-jw1qMebPMy"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='heart_disease')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZto63V-bPMy"
      },
      "source": [
        "dataset['heart_disease'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4-6gyAIbPMz"
      },
      "source": [
        "#### Ever Married\n",
        "\n",
        "El individuo estuvo o esta casado?. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnEvAmbSbPMz"
      },
      "source": [
        "dataset['ever_married'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUSmCybebPMz"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='ever_married')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIyH4B84bPMz"
      },
      "source": [
        "dataset['ever_married'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9ClpQl3bPM0"
      },
      "source": [
        "#### Work Type\n",
        "\n",
        "Tipo de trabajo. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nMwbdmFbPM0"
      },
      "source": [
        "dataset['work_type'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC-e96BDbPM0"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='work_type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHCL-dImbPM0"
      },
      "source": [
        "dataset['work_type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpkN1ImUbPM0"
      },
      "source": [
        "#### Residence type\n",
        "\n",
        "Tipo de zona donde reside el invididuo. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNf9XCcrbPM1"
      },
      "source": [
        "dataset['Residence_type'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zGIKlY1bPM1"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='Residence_type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCZSW-eZbPM1"
      },
      "source": [
        "dataset['Residence_type'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sAksWzjbPM1"
      },
      "source": [
        "#### AVG Glucose Level\n",
        "\n",
        "Nivel de grucosa medio el individuo. Es una variable numerica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAmKEQ14bPM1"
      },
      "source": [
        "dataset['avg_glucose_level'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_-ON24IbPM2"
      },
      "source": [
        "normal_size()\n",
        "sns.histplot(data=dataset, x='avg_glucose_level', kde=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06PSeW0QbPM2"
      },
      "source": [
        "#### BMI\n",
        "\n",
        "Es una variable numerica. Body Mass Index (Peso / Altura^2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2b0IdsVbPM2"
      },
      "source": [
        "normal_size()\n",
        "sns.histplot(data=dataset, x='bmi', kde= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPUbtq5NbPM2"
      },
      "source": [
        "#### Smoking Status\n",
        "\n",
        "Se refiere al nivel de fumador al que perteneces el individuo. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaLpdsJ3bPM2"
      },
      "source": [
        "dataset['smoking_status'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3BSCNyebPM3"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='smoking_status')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiZueofubPM3"
      },
      "source": [
        "dataset['smoking_status'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLShHMrhbPM3"
      },
      "source": [
        "#### Stroke\n",
        "\n",
        "Informa si el individuo sufrio un derrame cerebral o no. Es una variable categorica con los siguentes posibles valores:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btjL2tjJbPM3"
      },
      "source": [
        "dataset['stroke'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-WtYkd-bPM3"
      },
      "source": [
        "tiny_size()\n",
        "sns.histplot(data=dataset, y='stroke')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGuxt8T0bPM4"
      },
      "source": [
        "dataset['stroke'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN4pxTLlbPM4"
      },
      "source": [
        "#### b) Reportar si hay valores faltantes. ¬øCu√°ntos son y en qu√© atributos se encuentran? En caso de haberlos, ¬øes necesario y posible asignarles un valor?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLnNl-vmbPM4"
      },
      "source": [
        "Antes de completar valores faltante vamos a separar el dataset en los conjuntos en dos conjuntos, development, validation y test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxvJSYbWbPM4"
      },
      "source": [
        "def missing_values_summary(df):\n",
        "    result = round(df.isna().sum() * 100 / len(dataset), 2)\n",
        "    result = result[result > 0]\n",
        "    result = result.apply(lambda value: f'{value}%')\n",
        "    return result \n",
        "\n",
        "def set_summary(features, target, title=None):\n",
        "    if title:\n",
        "        print(f'\\n{title}:')\n",
        "    print('- Features shape:',  features.shape)\n",
        "    print('- Target shape:',     target.shape)\n",
        "    print('- Target classes:')\n",
        "    classes = target.value_counts(normalize=True)\n",
        "    values = classes * 100\n",
        "\n",
        "    print(\"\\t- Clase {}: {:.2f} %\".format(str(classes.index[0][0]), values.values[0]))\n",
        "    print(\"\\t- Clase {}: {:.2f} %\".format(str(classes.index[1][0]), values.values[1]))\n",
        "\n",
        "    missing = missing_values_summary(features)\n",
        "\n",
        "    if missing.empty:\n",
        "        print('- Features Missing values: No missing values!')\n",
        "    else:\n",
        "        print('- Features Missing values: ')\n",
        "        print(missing)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd5s4NyvbPM5"
      },
      "source": [
        "def num_column_names(df):\n",
        "    return df.select_dtypes(include=np.number).columns\n",
        "\n",
        "def cat_column_names(df):\n",
        "    return set(df.columns) - set(num_column_names(df))\n",
        "\n",
        "def cat_column_indexes(df):\n",
        "    return [df.columns.get_loc(col_name) for col_name in cat_column_names(df)]\n",
        "\n",
        "def unique_column_values(df, column_name): \n",
        "    return df[column_name].value_counts().index.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uciwra3xbPM5"
      },
      "source": [
        "@dataclass\n",
        "class SetsGroup:\n",
        "    dev_features: pd.DataFrame\n",
        "    test_features: pd.DataFrame\n",
        "    dev_target: pd.DataFrame\n",
        "    test_target: pd.DataFrame\n",
        "\n",
        "    def summary(self):\n",
        "        set_summary(self.dev_features, self.dev_target, title='Development Set')\n",
        "        set_summary(self.test_features, self.test_target, title='Test Set')\n",
        "\n",
        "    def features(self):\n",
        "        return pd.concat([self.dev_features, self.test_features])\n",
        "    \n",
        "    def cat_feature_names(self):\n",
        "        return cat_column_names(self.dev_features)\n",
        "\n",
        "    def cat_feature_indexes(self):\n",
        "        return cat_column_indexes(self.dev_features)\n",
        "\n",
        "    def num_feature_names(self):\n",
        "        return num_column_names(self.dev_features)\n",
        "\n",
        "    def feature_unique_values(self, column_name):\n",
        "        return unique_column_values(self.features(), column_name)\n",
        "\n",
        "    def dev_set(self):      \n",
        "        return pd.concat([sets_group.dev_features,sets_group.dev_target], axis=1)\n",
        "\n",
        "class DevTestSpliter:\n",
        "    @staticmethod\n",
        "    def split(\n",
        "        dataset, \n",
        "        target_col = 'stroke', \n",
        "        test_size = 0.2,\n",
        "        random_state = 1\n",
        "    ):\n",
        "        features = dataset.loc[:, dataset.columns != target_col]\n",
        "        target   = dataset[[target_col]]\n",
        "\n",
        "        dev_features, test_features, dev_target, test_target = train_test_split(\n",
        "            features, \n",
        "            target, \n",
        "            test_size    = test_size,\n",
        "            random_state = random_state,\n",
        "            stratify     = target.values\n",
        "        )\n",
        "        return SetsGroup(dev_features, test_features, dev_target, test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asNjqOCgbPM6"
      },
      "source": [
        "sets_group = DevTestSpliter.split(dataset, test_size = 0.2)\n",
        "sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMTQvupHbPM6"
      },
      "source": [
        "La unica variable que tiene valores incompletos es BMI y es un 3.93%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF8d3Rd2bPM6"
      },
      "source": [
        "Ahora completamos valores faltantes y comparamos la distribucion de la columna BMI antes y despues de la imputacion:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o10poElsbPM6"
      },
      "source": [
        "def impute_missing_values(df, excluded = ['id'], random_state=0):\n",
        "    num_features = df.select_dtypes(include=np.number)\n",
        "    num_features = num_features[set(num_features.columns) - set(excluded)]\n",
        "\n",
        "    # Algoritmo basado en el algoritmo MICE de R\n",
        "    imputer = IterativeImputer(random_state=random_state) \n",
        "    imputer.fit(num_features)\n",
        "\n",
        "    imp_num_features = imputer.transform(num_features)\n",
        "\n",
        "    result_df = pd.DataFrame(\n",
        "        data    = imp_num_features, \n",
        "        columns = num_features.columns\n",
        "    )\n",
        "    for col in cat_column_names(df):\n",
        "        result_df[col] = df[col].values\n",
        "\n",
        "    return result_df\n",
        "\n",
        "class MissingValuesImpoter:\n",
        "    def __init__(self, excluded = ['id'], random_state=0):\n",
        "        self.excluded     = excluded\n",
        "        self.random_state = random_state\n",
        "    \n",
        "    def impute(self, sets_group):\n",
        "        imp_dev_features  = impute_missing_values(\n",
        "            sets_group.dev_features,\n",
        "            random_state = self.random_state\n",
        "        )\n",
        "        imp_test_features = impute_missing_values(\n",
        "            sets_group.test_features,\n",
        "            random_state = self.random_state\n",
        "        )\n",
        "        return SetsGroup(\n",
        "            imp_dev_features, \n",
        "            imp_test_features, \n",
        "            sets_group.dev_target, \n",
        "            sets_group.test_target\n",
        "        )\n",
        "\n",
        "\n",
        "def compare_distributions(df1, df2, column):\n",
        "    sns.kdeplot(df1[column], shade=True, color=\"r\")\n",
        "    sns.kdeplot(df2[column], shade=True, color=\"b\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB5usAaybPM6"
      },
      "source": [
        "Chequeamos que se hayan completado als columnas y comparamos las distribuciones para development y test:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP45je9tbPM7"
      },
      "source": [
        "imputer = MissingValuesImpoter()\n",
        "imp_sets_group = imputer.impute(sets_group)\n",
        "imp_sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gSkoJi0bPM7"
      },
      "source": [
        "compare_distributions(imp_sets_group.dev_features, sets_group.dev_features, 'bmi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHGfORfHbPM7"
      },
      "source": [
        "compare_distributions(imp_sets_group.test_features, sets_group.test_features, 'bmi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT23ATFBbPM7"
      },
      "source": [
        "Ahora la columna BMI esta completa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t26WcfFDbPM7"
      },
      "source": [
        "#### c) ¬øQu√© variables se correlacionan m√°s con el evento de lesi√≥n (Stroke)? Para las cuatro m√°s correlacionadas, realizar un gr√°fico en el que se pueda observar la correlaci√≥n entre la variable y el stroke."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uzDUs0ybPM8"
      },
      "source": [
        "def heatmap(corr, resumed=True, figsize=(10, 10), vmax=.3, square=True, annot=True, linewidths=3):\n",
        "    if resumed:\n",
        "        mask = np.zeros_like(corr)\n",
        "        mask[np.triu_indices_from(mask)] = True\n",
        "    else:\n",
        "        mask = None\n",
        "\n",
        "    with sns.axes_style(\"white\"):\n",
        "        f, ax = plt.subplots(figsize=figsize)\n",
        "        ax = sns.heatmap(corr, mask=mask, vmax=vmax, square=square, annot=annot, linewidths=linewidths)\n",
        "\n",
        "        \n",
        "tmp_dataset = dataset.drop('id', axis=1)\n",
        "heatmap(tmp_dataset.corr())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgAPhIFobPM8"
      },
      "source": [
        "Al parecer stroke tiene baja correlacion con las demas variables pero existe un nivel. El orden de mas correlacionada a menos es:\n",
        "\n",
        "* Age (0.25)\n",
        "* Hypertension, Heart Disease y AVG Glucose Level (0.13)\n",
        "* BMI (0.044)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7vvYrBybPM8"
      },
      "source": [
        "**Edad vs. da√±oi cerebral**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q68-sK52bPM8"
      },
      "source": [
        "sns.pairplot(tmp_dataset[['age', 'stroke']], height=3)\n",
        "sns.pairplot(tmp_dataset[['age','stroke']], hue=\"stroke\", height=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDoiyli-bPM9"
      },
      "source": [
        "Conclusiones:\n",
        "\n",
        "* Ambas densidades estan solapadas.\n",
        "* Para age <= 23: La probabilidad de tener un da√±o cereblar es practicamente nula.\n",
        "* Para age > 23: La probabilidad de tener un da√±o cereblar es conciderable pero sigue siendo baja con relaciona a la probabilidad de no tener da√±o cerebral."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNMd61IBbPM9"
      },
      "source": [
        "**Hipertensi√≥n vs da√±o cerebral**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ2z-O7kbPM9"
      },
      "source": [
        "sns.pairplot(tmp_dataset[['hypertension', 'stroke']], height=3)\n",
        "sns.pairplot(tmp_dataset[['hypertension','stroke']], hue=\"stroke\", height=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aj-x6qIbPM9"
      },
      "source": [
        "Conclusiones:\n",
        "    \n",
        "* Ambas densidades estan solapadas.\n",
        "* -0.2 <= hypertension <= 0.2\n",
        "    * P(Da√±o cerebral/ Nivel de hipertension) <<<< P(Da√±o cerebral/ Nivel de hipertension)\n",
        "* 0.8 <= hypertension <= 1.2\n",
        "    * P(Da√±o cerebral/ Nivel de hipertension) <<<< P(No Da√±o cerebral/ Nivel de hipertension)\n",
        "* En otros valores hay una probabilidad muy baja de da√±o cereblar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psna_UMdbPM-"
      },
      "source": [
        "**Enfermedad del coraz√≥n vs da√±o cerebral**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpnjkbP5bPM-"
      },
      "source": [
        "sns.pairplot(tmp_dataset[['heart_disease', 'stroke']], height=3)\n",
        "sns.pairplot(tmp_dataset[['heart_disease','stroke']], hue=\"stroke\", height=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdIPQbGDbPM-"
      },
      "source": [
        "Conclusiones:\n",
        "    \n",
        "* Ambas densidades estan solapadas.\n",
        "* -0.2 <= heart_disease <= 0.2\n",
        "    * P(Da√±o cerebral/ Enfermedad del coraz√≥n) <<<< P(No Da√±o cerebral/ Enfermedad del coraz√≥n)\n",
        "* 0.8.5 <= hypertension <= 1.1\n",
        "    * P(Da√±o cerebral/ Enfermedad del coraz√≥n) <<<< P(No Da√±o cerebral/ Enfermedad del coraz√≥n)\n",
        "* En otros valores hay una probabilidad muy baja de da√±o cereblar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLZ7EpOpbPM-"
      },
      "source": [
        "**Promedio del nivel de glucosa vs da√±o cerebral**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKXUEbHBbPM-"
      },
      "source": [
        "sns.pairplot(tmp_dataset[['avg_glucose_level', 'stroke']], height=3)\n",
        "sns.pairplot(tmp_dataset[['avg_glucose_level','stroke']], hue=\"stroke\", height=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdqRfKBMbPM_"
      },
      "source": [
        "Conclusiones:\n",
        "    \n",
        "* Ambas densidades estan solapadas.\n",
        "* - 40 <= avg_glucose_level <= 170\n",
        "    * P(Da√±o cerebral/ nivel de glucosa) <<<< P(No Da√±o cerebral/ nivel de glucosa)\n",
        "* 0.8.5 <= hypertension <= 1.1\n",
        "    * P(Da√±o cerebral/ nivel de glucosa) <<<< P(No Da√±o cerebral/ nivel de glucosa)\n",
        "* En otros valores hay una probabilidad muy baja de da√±o cereblar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2kS1tqwbPM_"
      },
      "source": [
        "#### d) Se necesita saber cu√°les son los indicadores que determinan m√°s susceptibilidad a sufrir una lesi√≥n. ¬øQu√© atributos utilizar√° como variables predictoras? ¬øPor qu√©?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qaV8HlubPM_"
      },
      "source": [
        "Completar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nHicTnFbPM_"
      },
      "source": [
        "#### e) ¬øSe encuentra balanceado el conjunto de datos que utilizar√° para desarrollar el algoritmo dise√±ado para contestar el punto d)? En base a lo respondido, ¬øqu√© m√©tricas de performance reportar√≠a y por qu√©? En caso de estar desbalanceado, ¬øqu√© estrategia de balanceo utilizar√≠a?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKAgUH1WbPM_"
      },
      "source": [
        "imp_sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZQpv_IKbPM_"
      },
      "source": [
        "**Claramente esta desbalanceado** dado que es normal tener menos casos de da√±o cerebral."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeOQVL38bPNA"
      },
      "source": [
        "class OverUnderSampler:    \n",
        "    def __init__(\n",
        "        self,\n",
        "        categorical_features, \n",
        "        random_state=None, \n",
        "        oversampling_strategy='auto', \n",
        "        undersampling_strategy='auto'\n",
        "    ):\n",
        "        oversampler = SMOTENC(\n",
        "            categorical_features = categorical_features, \n",
        "            random_state = random_state,\n",
        "            sampling_strategy = undersampling_strategy\n",
        "        )\n",
        "        undersampler = RandomUnderSampler(sampling_strategy = undersampling_strategy)\n",
        "        self.pipeline = Pipeline(steps=[('oversampler', oversampler), ('undersampler', undersampler)])\n",
        "\n",
        "    def perform(self, features, target):\n",
        "        bal_features, bal_target = self.pipeline.fit_resample(features.values, target.values)\n",
        "        \n",
        "        # Matrix to DataFrame\n",
        "        bal_features = pd.DataFrame(data = bal_features, columns = features.columns)\n",
        "        bal_target   = pd.DataFrame(data = bal_target,   columns = target.columns)\n",
        "\n",
        "        return bal_features, bal_target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "oLgdstO_bPNA"
      },
      "source": [
        "class SetsGroupOverUnderSampler:\n",
        "    @staticmethod\n",
        "    def createFromHps(categorical_features, hps):\n",
        "        return SetsGroupOverUnderSampler(\n",
        "            categorical_features   = categorical_features,\n",
        "            random_state           = hps.random_state, \n",
        "            oversampling_strategy  = hps.oversampling_strategy,\n",
        "            undersampling_strategy = hps.undersampling_strategy \n",
        "        )\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        categorical_features,\n",
        "        random_state = None, \n",
        "        oversampling_strategy = 0.1,\n",
        "        undersampling_strategy = 1\n",
        "    ):\n",
        "        self.sampler = OverUnderSampler(\n",
        "            categorical_features   = categorical_features,\n",
        "            random_state           = random_state, \n",
        "            oversampling_strategy  = oversampling_strategy,\n",
        "            undersampling_strategy = undersampling_strategy\n",
        "        )\n",
        "\n",
        "    def perform(self, sets_group):\n",
        "        bal_dev_features, bal_dev_target = self.sampler.perform(\n",
        "            sets_group.dev_features, \n",
        "            sets_group.dev_target\n",
        "        )\n",
        "\n",
        "        return SetsGroup(\n",
        "            bal_dev_features, \n",
        "            sets_group.test_features, \n",
        "            bal_dev_target, \n",
        "            sets_group.test_target\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rq38pk4nY6-"
      },
      "source": [
        "sampler = SetsGroupOverUnderSampler(\n",
        "    categorical_features   = imp_sets_group.cat_feature_indexes(),\n",
        "    oversampling_strategy  = 0.1,\n",
        "    undersampling_strategy = 1\n",
        ")\n",
        "oversampled_sets_group = sampler.perform(imp_sets_group)\n",
        "oversampled_sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwA3ixDQbPNA"
      },
      "source": [
        "#### f) Suponiendo que es m√°s importante detectar los casos en donde el evento ocurre. ¬øQu√© medida de performance utilizar√≠a? Si utiliza FŒ≤-Score, ¬øqu√© valor de Œ≤ eligir√≠a?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZpTMGSJbPNB"
      },
      "source": [
        "**Completar!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZJtYo5vbPNB"
      },
      "source": [
        "#### g) Implementar el algoritmo introducido en el punto d) utilizando √°rboles de decisi√≥n. En primer lugar, se deber√° separar un 20% de los datos para usarlos como conjunto de evaluaci√≥n (test set). El conjunto restante (80%) es el de desarrollo y es con el que se deber√° continuar haciendo el trabajo. Realizar los siguientes puntos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xF3X_IbbPNB"
      },
      "source": [
        "class CategoricalFeaturesEncoder:\n",
        "    def perform(self, sets_group):\n",
        "        enc_dev_features  = sets_group.dev_features.copy()\n",
        "        enc_test_features = sets_group.test_features.copy()\n",
        " \n",
        "        for col_name in sets_group.cat_feature_names():            \n",
        "            encoder = LabelEncoder()\n",
        "            encoder.fit(sets_group.feature_unique_values(col_name))\n",
        "\n",
        "            enc_test_features[col_name] = encoder.transform(sets_group.test_features[col_name].values)\n",
        "            enc_dev_features[col_name]  = encoder.transform(sets_group.dev_features[col_name].values)\n",
        "\n",
        "\n",
        "        return SetsGroup(\n",
        "            enc_dev_features, \n",
        "            enc_test_features, \n",
        "            sets_group.dev_target, \n",
        "            sets_group.test_target\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTD2QZwLIR4-"
      },
      "source": [
        "encoder = CategoricalFeaturesEncoder()\n",
        "encoded_sets_group = encoder.perform(oversampled_sets_group)\n",
        "encoded_sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-PxU2A-bPNB"
      },
      "source": [
        "encoder = CategoricalFeaturesEncoder()\n",
        "encoded_sets_group = encoder.perform(encoded_sets_group)\n",
        "encoded_sets_group.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEgOG8fuIAHB"
      },
      "source": [
        "def features_importance(features, target):\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(features, target)\n",
        "    importance = model.feature_importances_\n",
        "\n",
        "    # Orden desc...\n",
        "    return np.sort(importance)[::-1]\n",
        "\n",
        "def plot_features_importance(sets_group):\n",
        "    importance = features_importance(\n",
        "        sets_group.dev_features.values, \n",
        "        sets_group.dev_target.values\n",
        "    )\n",
        "\n",
        "    column_names = sets_group.dev_features.columns\n",
        "    plt.bar([column_names[x] for x in range(len(importance))], importance)\n",
        "    plt.show()\n",
        "\n",
        "big_size()\n",
        "plot_features_importance(encoded_sets_group)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lewTlbk81kvH"
      },
      "source": [
        "Quitamos los features menos importantes para la clasificacion por stroke:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWMtPA_N2BlU"
      },
      "source": [
        "def remove_columns(df, columns): df.drop(columns, axis = 1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSAWjJxnIt9P"
      },
      "source": [
        "less_importante_features = [\n",
        "    'Residence_type',\n",
        "    'smoking_status',\n",
        "    'gender',\n",
        "    'ever_married',\n",
        "    'work_type'\n",
        "]\n",
        "\n",
        "remove_columns(encoded_sets_group.dev_features, less_importante_features)\n",
        "remove_columns(encoded_sets_group.test_features, less_importante_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNprI7AmbPNC"
      },
      "source": [
        "def to_tree_img(tree):\n",
        "    dot_data = StringIO()\n",
        "    export_graphviz(\n",
        "        tree, \n",
        "        out_file            = dot_data,  \n",
        "        filled              = True,\n",
        "        rounded            = True,\n",
        "        special_characters = True\n",
        "    )\n",
        "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
        "    graph.write_png('tree.png')\n",
        "    return mpimg.imread('tree.png')\n",
        "\n",
        "def plot_tree(tree):\n",
        "    sns.set(rc={'figure.figsize':(15, 8)})\n",
        "    plt.title('Arbol de decisi√≥n')\n",
        "    plt.grid(False)\n",
        "    plt.imshow(to_tree_img(tree))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "def cm_plot(ax, cm, title='Matriz de confusi√≥n'):     \n",
        "    sns.heatmap(\n",
        "        cm, \n",
        "        ax=ax,\n",
        "        annot=True, \n",
        "        fmt='g', \n",
        "        cmap='Blues', \n",
        "        cbar=False\n",
        "    )\n",
        "    ax.set_ylabel('Realidad')\n",
        "    ax.set_xlabel('Predicciones')\n",
        "    ax.set_title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO1KhYKobPNC"
      },
      "source": [
        "@dataclass\n",
        "class HiperParams:\n",
        "    # Hiper parametros del modelo:\n",
        "    criterion: any = 'entropy' \n",
        "    max_depth: int = 5\n",
        "    min_samples_leaf: int = 1\n",
        "    ccp_alpha: float = 0.0\n",
        "    class_weight: any = 'balanced'\n",
        "    # Semilla usada en todos lo algoritmos.\n",
        "    random_state: int = 42 \n",
        "    # Hiper parametros para over/up sampling:\n",
        "    # Porcentaje de ejemplos duplicados en la calse minoritaria.\n",
        "    oversampling_strategy: float = 0.1\n",
        "    # Porcentaje de ejemplos removidos ne la clase mayoritaria.\n",
        "    undersampling_strategy: float = 1\n",
        "\n",
        "class ModelFactory:\n",
        "    @staticmethod\n",
        "    def create(hps):\n",
        "        tree = DecisionTreeClassifier(\n",
        "            criterion        = hps.criterion,\n",
        "            max_depth        = hps.max_depth,\n",
        "            min_samples_leaf = hps.min_samples_leaf,\n",
        "            ccp_alpha        = hps.ccp_alpha,\n",
        "            class_weight     = hps.class_weight,\n",
        "            random_state     = hps.random_state\n",
        "        )\n",
        "        return Model(tree, hps)\n",
        "\n",
        "    def create_from(tree):\n",
        "        return Model(tree, hps)\n",
        "\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, tree, hps):\n",
        "        self.tree = tree\n",
        "        self.hps = hps\n",
        "\n",
        "    def fit(self, sets_group):\n",
        "        return self.tree.fit(\n",
        "            sets_group.dev_features.values, \n",
        "            sets_group.dev_target.values\n",
        "        )\n",
        "\n",
        "    def predict(self, features):\n",
        "        return self.tree.predict(features.values)\n",
        "\n",
        "    def evaluate(self, sets_group):\n",
        "        train_pred  = self.predict(sets_group.dev_features)\n",
        "        test_pred   = self.predict(sets_group.test_features)\n",
        "        return ModelSummary(\n",
        "            sets_group, \n",
        "            train_pred, \n",
        "            test_pred, \n",
        "            tree = self.tree, \n",
        "            hps  = self.hps\n",
        "        )\n",
        "\n",
        "class ModelSummary:\n",
        "    def __init__(self, sets_group, train_pred, test_pred, tree, hps):\n",
        "        self.metrics = {\n",
        "            'train_accuracy':         accuracy_score(sets_group.dev_target.values, train_pred),\n",
        "            'train_precision':        precision_score(sets_group.dev_target.values, train_pred),\n",
        "            'train_recall':           recall_score( sets_group.dev_target.values, train_pred),\n",
        "            \"train_f1_score\":         f1_score(sets_group.dev_target.values, train_pred),\n",
        "            'train_confusion_matrix': confusion_matrix(sets_group.dev_target.values, train_pred),\n",
        "\n",
        "            'test_accuracy':          accuracy_score(sets_group.test_target.values, test_pred),\n",
        "            'test_precision':         precision_score(sets_group.test_target.values, test_pred),\n",
        "            'test_recall':            recall_score(sets_group.test_target.values, test_pred),\n",
        "            \"test_f1_score\":          f1_score(sets_group.test_target.values, test_pred),\n",
        "            'test_confusion_matrix':  confusion_matrix(sets_group.test_target.values, test_pred)\n",
        "        }\n",
        "        self.tree = tree\n",
        "        self.hps = hps\n",
        "\n",
        "    def plot_confusion_matrix(self):\n",
        "        sns.set(rc={'figure.figsize':(7, 4)})\n",
        "        fig = plt.figure()\n",
        "        gs = fig.add_gridspec(1, 2, hspace=0.1, wspace=0.1)\n",
        "        (ax1, ax2) = gs.subplots(sharex='col', sharey='row')\n",
        "        cm_plot(\n",
        "            ax=ax1,\n",
        "            cm=self.metrics['train_confusion_matrix'],\n",
        "            title='Entrenamiento'\n",
        "        )\n",
        "        cm_plot(\n",
        "            ax=ax2,\n",
        "            cm=self.metrics['test_confusion_matrix'],\n",
        "            title='Test'\n",
        "        )\n",
        "        fig.suptitle('Matriz de confusi√≥n')\n",
        "        plt.show()\n",
        "\n",
        "    def show_metrics(self):\n",
        "        print('\\nMetricas:')\n",
        "        for name in self.metrics.keys():\n",
        "            if \"confusion_matrix\" not in name:\n",
        "                print(f'- {name}: {self.metrics[name]*100:.2f}%')\n",
        "\n",
        "    def showHps(self):\n",
        "        if self.hps:\n",
        "            print('\\nHiper Parametros:\\n-', self.hps)\n",
        "\n",
        "    def show(self):\n",
        "        self.showHps()\n",
        "        self.show_metrics()\n",
        "        self.plot_confusion_matrix() \n",
        "        plot_tree(self.tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaUARJap1G-u"
      },
      "source": [
        "def data_transform_pipeline(data, hps):\n",
        "    # Imputamos valores faltantes (MICE like)...\n",
        "    imputer = MissingValuesImpoter(random_state = hps.random_state)\n",
        "    data = imputer.impute(data)\n",
        "    \n",
        "    # Balanceamos el dataset (over/up sampling)...\n",
        "    sampler = SetsGroupOverUnderSampler.createFromHps(\n",
        "        data.cat_feature_indexes(), \n",
        "        hps\n",
        "    )\n",
        "    data = sampler.perform(data)\n",
        "\n",
        "    # Llevamos variables categoricas a numericas...\n",
        "    endcoder = CategoricalFeaturesEncoder()\n",
        "    data = endcoder.perform(data)\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpUN03FdbPNC"
      },
      "source": [
        "hiper_params = [\n",
        "    # Usamos todos los valores por defecto...\n",
        "    HiperParams(\n",
        "        # Hiper parametros del modelo:\n",
        "        criterion        = 'entropy',\n",
        "        max_depth        = 2,\n",
        "        min_samples_leaf = 100\n",
        "    )\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjsf4R2RbPND"
      },
      "source": [
        "for hps in hiper_params:\n",
        "    data = data_transform_pipeline(sets_group, hps)\n",
        "\n",
        "    # Resumen de estrutura de los datos...\n",
        "    data.summary()\n",
        "\n",
        "    # Creamos el modelo (Arbol de decisi√≥n)...\n",
        "    model = ModelFactory.create(hps)\n",
        "\n",
        "    # Entrenamos...\n",
        "    model.fit(data)\n",
        "    summary = model.evaluate(data)\n",
        "    \n",
        "    # Mostramos el resumen de metricas...\n",
        "    summary.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykBZ1JFsbPND"
      },
      "source": [
        "#### g.1) Armar conjuntos de entrenamiento y validaci√≥n con proporci√≥n 80-20 del conjunto de desarrollo de forma aleatoria. Usar 50 semillas distintas y realizar un gr√°fico de caja y bigotes que muestre c√≥mo var√≠a la m√©trica elegida en c) en esas 50 particiones distintas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXDSqSQ79W-o"
      },
      "source": [
        "class MetricsComparePlot:\n",
        "    def __init__(self):\n",
        "        self.accs = []\n",
        "        self.precisions = []\n",
        "        self.recalls = []\n",
        "        self.f1s = []\n",
        "\n",
        "    def add(self, y_val,y_pred_val):\n",
        "        self.accs.append(accuracy_score(y_val,y_pred_val))\n",
        "        self.precisions.append(precision_score(y_val,y_pred_val))\n",
        "        self.recalls.append(recall_score(y_val,y_pred_val))\n",
        "        self.f1s.append(f1_score(y_val,y_pred_val))\n",
        "\n",
        "    def plot(self):\n",
        "        all_metrics = self.accs + self.precisions + self.recalls + self.f1s\n",
        "        metric_labels = ['Accuracy']*len(self.accs) + ['Precision']*len(self.precisions) + ['Recall']*len(self.recalls) + ['F1 Score']*len(self.f1s)\n",
        "        sns.set_context('talk')\n",
        "        plt.figure(figsize=(15,8))\n",
        "        sns.boxplot(metric_labels, all_metrics)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3xV0j6Dpmfz"
      },
      "source": [
        "# Rearmo dataset dev (feat+tgt) y vuelvo a correr train/test \n",
        "# split, ya que el primero separ√≥ dev + val\n",
        "dev_concat = sets_group.dev_set()\n",
        "\n",
        "n_seeds   = 50\n",
        "test_size = 0.2\n",
        "plotter   = MetricsComparePlot()\n",
        "\n",
        "for seed in range(n_seeds):\n",
        "    hps = HiperParams(random_state = seed)\n",
        "\n",
        "    sets_group_dev = DevTestSpliter.split(\n",
        "        dev_concat, \n",
        "        test_size = test_size, \n",
        "        random_state = hps.random_state\n",
        "    )\n",
        "\n",
        "    encoded_sets_group_dev = data_transform_pipeline(sets_group_dev, hps)\n",
        "\n",
        "    arbol = ModelFactory.create(hps)\n",
        "    arbol = arbol.fit(encoded_sets_group_dev)\n",
        "    y_pred_val = arbol.predict(encoded_sets_group_dev.test_features)\n",
        "\n",
        "    plotter.add(encoded_sets_group_dev.test_target, y_pred_val)\n",
        "\n",
        "plotter.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d6YBNB2bPND"
      },
      "source": [
        "#### g.2) Usar validaci√≥n cruzada de 50 iteraciones (50-fold cross validation). Realizar un gr√°fico de caja y bigotes que muestre c√≥mo var√≠a la m√©trica elegida en esas 50 particiones distintas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6sLNnKhhPDu"
      },
      "source": [
        "def search_best_model(sets_group, params_grid, n_splits, n_iter):\n",
        "    print('Random Search Hiper-params:', params_grid)\n",
        "\n",
        "    #¬†scoring = make_scorer(f1_score)\n",
        "    scoring = make_scorer(fbeta_score, beta=0.2)\n",
        "\n",
        "    randomcv = RandomizedSearchCV(\n",
        "        estimator           = DecisionTreeClassifier(),\n",
        "        param_distributions = params_grid,\n",
        "        scoring             = scoring,\n",
        "        cv                  = StratifiedKFold(n_splits=n_splits),\n",
        "        n_iter              = n_iter,\n",
        "        return_train_score  = True\n",
        "    )\n",
        "    randomcv.fit(\n",
        "        sets_group.dev_features.values,\n",
        "        sets_group.dev_target.values\n",
        "    )\n",
        "    \n",
        "    random_cv_results = pd.DataFrame(randomcv.cv_results_)\n",
        "    print('Models Count:', random_cv_results.shape[0])\n",
        "\n",
        "    return randomcv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMvFA0ULcmv0"
      },
      "source": [
        "def plot_randomcv_metrics(randomcv_result, metrics):\n",
        "    df = pd.DataFrame(randomcv_result.cv_results_)\n",
        "    normal_size()\n",
        "    for metric in metrics:\n",
        "        plt.plot(df[metric],  linestyle='-', marker='o', label=metric)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_randomcv_score(randomcv_result):\n",
        "    plot_randomcv_metrics(randomcv_result, metrics=['mean_train_score','mean_test_score'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKLoVTOQpySK"
      },
      "source": [
        "n_splits = 50\n",
        "\n",
        "randomcv = search_best_model(\n",
        "    encoded_sets_group_dev,\n",
        "    params_grid = {\n",
        "        'criterion': ['gini','entropy'],\n",
        "        'max_depth': list(range(1, 50)),\n",
        "        'min_samples_leaf': list(range(1, 50)),\n",
        "        'class_weight': ['balanced']\n",
        "    }, \n",
        "    n_splits = n_splits,\n",
        "    n_iter = 50\n",
        ")\n",
        "plot_randomcv_score(randomcv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8PFbjTjdSVI"
      },
      "source": [
        "random_cv_results = pd.DataFrame(randomcv.cv_results_)\n",
        "random_cv_results[random_cv_results.params == randomcv.best_params_]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEwsnTE_pyYT"
      },
      "source": [
        "metric_labels = []\n",
        "metric_labels = ['F1 Score']*n_splits\n",
        "\n",
        "accs_kfold = []\n",
        "for x in range(0,n_splits):\n",
        "  col = 'split' + str(x) + '_test_score'\n",
        "  accs_kfold.append(random_cv_results[col].values[0])\n",
        "\n",
        "hue = []\n",
        "hue = ['Arbol1']*n_splits\n",
        "sns.set_context('talk')\n",
        "plt.figure(figsize=(15,8))\n",
        "sns.boxplot(metric_labels,accs_kfold,hue=hue)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UlOsS_KbPND"
      },
      "source": [
        "#### h) Graficar el √°rbol de decisi√≥n con mejor performance encontrado en el punto g2). Analizar el √°rbol de decisi√≥n armado (atributos elegidos y decisiones evaluadas)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsDLc4ZnCYbj"
      },
      "source": [
        "def evaluate_model(estimator, sets_group):\n",
        "    model = ModelFactory.create_from(estimator)\n",
        "    model.fit(sets_group)\n",
        "    summary = model.evaluate(sets_group)\n",
        "    summary.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIF_za85SSdv"
      },
      "source": [
        "evaluate_model(\n",
        "    estimator  = randomcv.best_estimator_,\n",
        "    sets_group = encoded_sets_group_dev\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-uUf3qbbPNE"
      },
      "source": [
        "#### i) Usando validaci√≥n cruzada de 10 iteraciones (10-fold cross validation), probar distintos valores de Œ± del algoritmo de poda m√≠nima de complejidad de costos (algoritmo de poda de sklearn). Hacer gr√°ficos de la performance en validaci√≥n y entrenamiento en funci√≥n del Œ±. Explicar c√≥mo var√≠a la profundidad de los √°rboles al realizar la poda con distintos valores de Œ±."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xY8qa4hr2bl"
      },
      "source": [
        "def to_params_grid(hiper_params):\n",
        "    params_grid = {}\n",
        "    for name in hiper_params.keys():\n",
        "        params_grid[name] = [hiper_params[name]]\n",
        "    return params_grid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzDhUBJQYeLc"
      },
      "source": [
        "params_grid = to_params_grid(randomcv.best_params_)\n",
        "params_grid['ccp_alpha'] = np.linspace(0, 0.3, 100)\n",
        "\n",
        "randomcv2 = search_best_model(\n",
        "    encoded_sets_group_dev,\n",
        "    params_grid = params_grid, \n",
        "    n_splits = 10,\n",
        "    n_iter = 5\n",
        ")\n",
        "\n",
        "plot_randomcv_score(randomcv2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24_SJzGlbPNE"
      },
      "source": [
        "#### j) Evaluar en el conjunto de evaluaci√≥n, el √°rbol correspondiente al Œ± que maximice la performance en el conjunto de validaci√≥n. Comparar con el caso sin poda (Œ±=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhhG2w6At7f-"
      },
      "source": [
        "**Modelo con poda:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk30vqcAjEvu"
      },
      "source": [
        "evaluate_model(\n",
        "    estimator  = randomcv2.best_estimator_,\n",
        "    sets_group = encoded_sets_group_dev\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgQBbax5uH-N"
      },
      "source": [
        "**Modelo sin poda**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjgB_P_zuDpg"
      },
      "source": [
        "evaluate_model(\n",
        "    estimator  = randomcv.best_estimator_,\n",
        "    sets_group = encoded_sets_group_dev\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH9O_YiYbPNE"
      },
      "source": [
        "#### k) Para el √°rbol sin poda, obtener la importancia de los descriptores usando la t√©cnica de eliminaci√≥n recursiva. Reentrenar el √°rbol usando s√≥lo los 3 descriptores m√°s importantes. Comparar la performance en el conjunto de prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTL_a1A9bPNE"
      },
      "source": [
        "**Completar!**"
      ]
    }
  ]
}